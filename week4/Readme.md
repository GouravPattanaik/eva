- total number of parameters---14,112
- final validation/test accuracy---99.54%


## Logs:
epoch=1 loss=0.24695727229118347 batch_id=937: 100%|█████████████████████████████████| 938/938 [00:17<00:00, 53.41it/s]

Epoch 1--Train set: Average loss: 0.2470, Training Accuracy: 50740/60000 (84.57%)


Epoch 1--Test set: Average loss: 0.0878, Validation Accuracy: 9805/10000 (98.05%)

epoch=2 loss=0.10117413103580475 batch_id=937: 100%|█████████████████████████████████| 938/938 [00:20<00:00, 44.78it/s]

Epoch 2--Train set: Average loss: 0.1012, Training Accuracy: 57848/60000 (96.41%)


Epoch 2--Test set: Average loss: 0.0527, Validation Accuracy: 9857/10000 (98.57%)

epoch=3 loss=0.05008406937122345 batch_id=937: 100%|█████████████████████████████████| 938/938 [00:20<00:00, 46.56it/s]

Epoch 3--Train set: Average loss: 0.0501, Training Accuracy: 58466/60000 (97.44%)


Epoch 3--Test set: Average loss: 0.0373, Validation Accuracy: 9890/10000 (98.90%)

epoch=4 loss=0.0396733433008194 batch_id=937: 100%|██████████████████████████████████| 938/938 [00:21<00:00, 44.29it/s]

Epoch 4--Train set: Average loss: 0.0397, Training Accuracy: 58661/60000 (97.77%)


Epoch 4--Test set: Average loss: 0.0331, Validation Accuracy: 9902/10000 (99.02%)

epoch=5 loss=0.009025141596794128 batch_id=937: 100%|████████████████████████████████| 938/938 [00:21<00:00, 44.38it/s]

Epoch 5--Train set: Average loss: 0.0090, Training Accuracy: 58782/60000 (97.97%)


Epoch 5--Test set: Average loss: 0.0305, Validation Accuracy: 9905/10000 (99.05%)

epoch=6 loss=0.04769548773765564 batch_id=937: 100%|█████████████████████████████████| 938/938 [00:21<00:00, 43.98it/s]

Epoch 6--Train set: Average loss: 0.0477, Training Accuracy: 58883/60000 (98.14%)


Epoch 6--Test set: Average loss: 0.0275, Validation Accuracy: 9914/10000 (99.14%)

epoch=7 loss=0.033467262983322144 batch_id=937: 100%|████████████████████████████████| 938/938 [00:20<00:00, 45.82it/s]

Epoch 7--Train set: Average loss: 0.0335, Training Accuracy: 58990/60000 (98.32%)


Epoch 7--Test set: Average loss: 0.0253, Validation Accuracy: 9923/10000 (99.23%)

epoch=8 loss=0.06352145969867706 batch_id=937: 100%|█████████████████████████████████| 938/938 [00:21<00:00, 42.98it/s]

Epoch 8--Train set: Average loss: 0.0635, Training Accuracy: 58975/60000 (98.29%)


Epoch 8--Test set: Average loss: 0.0253, Validation Accuracy: 9917/10000 (99.17%)

epoch=9 loss=0.09636054933071136 batch_id=937: 100%|█████████████████████████████████| 938/938 [00:20<00:00, 44.70it/s]

Epoch 9--Train set: Average loss: 0.0964, Training Accuracy: 59030/60000 (98.38%)


Epoch 9--Test set: Average loss: 0.0223, Validation Accuracy: 9931/10000 (99.31%)

epoch=10 loss=0.01684170961380005 batch_id=937: 100%|████████████████████████████████| 938/938 [00:21<00:00, 42.92it/s]

Epoch 10--Train set: Average loss: 0.0168, Training Accuracy: 59097/60000 (98.50%)


Epoch 10--Test set: Average loss: 0.0214, Validation Accuracy: 9933/10000 (99.33%)

epoch=11 loss=0.06998667120933533 batch_id=937: 100%|████████████████████████████████| 938/938 [00:21<00:00, 44.58it/s]

Epoch 11--Train set: Average loss: 0.0700, Training Accuracy: 59086/60000 (98.48%)


Epoch 11--Test set: Average loss: 0.0227, Validation Accuracy: 9930/10000 (99.30%)

epoch=12 loss=0.24019618332386017 batch_id=937: 100%|████████████████████████████████| 938/938 [00:21<00:00, 43.02it/s]

Epoch 12--Train set: Average loss: 0.2402, Training Accuracy: 59111/60000 (98.52%)


Epoch 12--Test set: Average loss: 0.0205, Validation Accuracy: 9943/10000 (99.43%)

epoch=13 loss=0.17071953415870667 batch_id=937: 100%|████████████████████████████████| 938/938 [00:21<00:00, 43.51it/s]

Epoch 13--Train set: Average loss: 0.1707, Training Accuracy: 59157/60000 (98.59%)


Epoch 13--Test set: Average loss: 0.0189, Validation Accuracy: 9949/10000 (99.49%)

epoch=14 loss=0.041865766048431396 batch_id=937: 100%|███████████████████████████████| 938/938 [00:21<00:00, 44.55it/s]

Epoch 14--Train set: Average loss: 0.0419, Training Accuracy: 59237/60000 (98.73%)


Epoch 14--Test set: Average loss: 0.0224, Validation Accuracy: 9923/10000 (99.23%)

epoch=15 loss=0.036569446325302124 batch_id=937: 100%|███████████████████████████████| 938/938 [00:21<00:00, 43.28it/s]

Epoch 15--Train set: Average loss: 0.0366, Training Accuracy: 59190/60000 (98.65%)


Epoch 15--Test set: Average loss: 0.0196, Validation Accuracy: 9940/10000 (99.40%)

epoch=16 loss=0.04074643552303314 batch_id=937: 100%|████████████████████████████████| 938/938 [00:21<00:00, 43.41it/s]

Epoch 16--Train set: Average loss: 0.0407, Training Accuracy: 59251/60000 (98.75%)


Epoch 16--Test set: Average loss: 0.0198, Validation Accuracy: 9944/10000 (99.44%)

epoch=17 loss=0.011231973767280579 batch_id=937: 100%|███████████████████████████████| 938/938 [00:21<00:00, 43.52it/s]

Epoch 17--Train set: Average loss: 0.0112, Training Accuracy: 59259/60000 (98.77%)


Epoch 17--Test set: Average loss: 0.0189, Validation Accuracy: 9942/10000 (99.42%)

epoch=18 loss=0.008846849203109741 batch_id=937: 100%|███████████████████████████████| 938/938 [00:21<00:00, 44.03it/s]

Epoch 18--Train set: Average loss: 0.0088, Training Accuracy: 59313/60000 (98.86%)


Epoch 18--Test set: Average loss: 0.0178, Validation Accuracy: 9947/10000 (99.47%)

epoch=19 loss=0.04289734363555908 batch_id=937: 100%|████████████████████████████████| 938/938 [00:21<00:00, 42.78it/s]

Epoch 19--Train set: Average loss: 0.0429, Training Accuracy: 59279/60000 (98.80%)


Epoch 19--Test set: Average loss: 0.0195, Validation Accuracy: 9937/10000 (99.37%)

epoch=20 loss=0.001962423324584961 batch_id=937: 100%|███████████████████████████████| 938/938 [00:21<00:00, 42.83it/s]

Epoch 20--Train set: Average loss: 0.0020, Training Accuracy: 59311/60000 (98.85%)


Epoch 20--Test set: Average loss: 0.0167, Validation Accuracy: 9954/10000 (99.54%)

#### Here is the step-by-step approach, being taken to achieve the required goals (meeting 99.4%+ accuracy, under the required parameters: <=20K and within 20 epochs) :
 
Fix up the required architecture/layer-organization...in terms of choosing the number of kernels for each layer, am trying to follow a 'multiple-of-8' numbers (from 8, 16, 24, 32...etc):

 Architecture will have 3 "components":
 
 1. One initial,"image-conv2d" layer at the begining, to convolve over the "original image" channels, am 
    initially providing 8 number of kernels for this 'separate' layer (which feeds in to the next  
    "2-conv2d-layer-block").It needs to be noted that this 1 initial layer + The 2 following layers(for
    "2-conv2d-layer-block") provide receptive field of 7x7 pixels(3->5->7) sufficient for the MNIST datset's
    edges & gradient generation.In this evolution-experiment, kernel numbers numbers started out as 8 initially, 
    but in the final architecture(which met the requirements) it became 16.
    
	
 2. conv2d-BLOCK with 2 layers (in this case):
    This block will be placed after the first "image-conv2d" layer, and one more instance of this block, will 
    also follow the transition-block (explained below) later.
    In this evolution-experiment, kernel numbers numbers initially started out as (8-16) for the 'first-2-layer-block'
    & (8-16) for the 'second-2-layer-block', but in the final architecture(which met the requirements) it
    became (16-16) for the 'first-2-layer-block' & (24-24) for the 'second-2-layer-block'.
    
    
 3. Transition Blocks:
    1st transition layer, with both max-pool(k=2,s=2) and a-1x1-feature-merger kernel, following the
    'first-2-layer-block'.
    2nd transition layer, towards the end (following the 2nd conv2d-block) which does NOT have
    the maxpool (i.e. just has one 1x1-feature-merger operator), and followed by the Global
    Average Pooling (GAP) layer leading upto the Softmax layer.    
    Here, at the end, we have another 'organization-possibility' i.e. we can also have a GAP layer followed
    by a 1x1 operator (which actually resembles a fully-connected(FC) layer in this case. (Note: for my 
    experiments, am finding that the 1x1, followed by GAP gave BETTER results, as compared to GAP followed
    by 1x1(in a FC-way).
    Hence, will be showing this evolution of incremental changes to 1x1->GAP organization rather than 
    GAP->1x1 (though, the first 2 networks(NW) below show both of them, but later iterations build upon
    the basic-NW having '1x1->GAP' organization only)


### Architecture (i.e. in terms of channels used across individual layers):

    
    i.   "image-conv2d" layer: o/p initially 8 channels (becomes 16 in the final one)
    
    ii.  2 similar conv2d blocks, with:
    
              1st layer: (8-16) o/p channels (becomes (16-16) in the final one)
			 
              2nd layer: (8-16) o/p channels (becomes (24-24) in the final one)
	      
    iii. 1x1 conv2d for 2nd transition-layer: 10 o/p channels(for num-classes=10 digits)
    
 
Input Channels/Image  |  Conv2d/Transform      | Output Channels | RF
---------------------|--------------|----------------------|----------------------
`28x28x1`              | `(3x3x1)x8`   |      `26x26x8`  |      `3x3`|      
` `              | `ReLU`   |      ` `  |      ` ` 
**26x26x8**             | **(3x3x8)x8**  |      **24x24x8** |      **5x5**
** **             | **ReLU**   |     ** **  |     ** **      
**24x24x8**             | **(3x3x8)x16**  |      **22x22x16** |      **7x7**  
** **             | **ReLU**   |     ** **  |     ** **                       
*22x22x16*             |   *MP(2x2)*    |      *11x11x16*   |      *8x8*                      
*11x11x16*             | *(1x1x16)x8*  |      *11x11x8*    |      *8x8* 
** **             | *ReLU*   |     * *   |     * *
**11x11x8**             | **(3x3x8)x8**  |      **9x9x8** |      **12x12** 
** **             | **ReLU**   |     ** **  |     ** **   
**9x9x8**               | **(3x3x8)x16**  |      **7x7x16**  |      **16x16** 
** **             | **ReLU**   |     ** **  |     ** **    
*7x7x16*               | *(1x1x16)x10*  |      *7x7x10*    |      *16x16*  (NO RELU at the o/p of this layer)    
7x7x10               | GAP  LAYER   |      1x10          |


    iv. The 2nd variant could've been, as below, where the last 1x1 is actually behaving like a FC-layer:

Input Channels/Image  |  Conv2d/Transform      | Output Channels | RF
---------------------|--------------|----------------------|----------------------
`28x28x1`              | `(3x3x1)x8`   |      `26x26x8`  |      `3x3`
` `              | `ReLU`   |      ` `  |      ` `  
**26x26x8**             | **(3x3x8)x8**  |      **24x24x8** |      **5x5**  
** **             | **ReLU**   |     ** **  |     ** **      
**24x24x8**             | **(3x3x8)x16**  |      **22x22x16** |      **7x7**  
** **             | **ReLU**   |     ** **  |     ** **      
*22x22x16*             |   *MP(2x2)*    |      *11x11x16*   |      *8x8*                      
*11x11x16*             | *(1x1x16)x8*  |      *11x11x8*    |      *8x8* 
 ** **             | *ReLU*   |     * *   |     * *      
**11x11x8**             | **(3x3x8)x8**  |      **9x9x8** |      **12x12**
** **             | **ReLU**   |     ** **  |     ** **   
**9x9x8**               | **(3x3x8)x16**  |      **7x7x16**  |      **16x16** (NO RELU at the o/p of this layer)            
7x7x16               | GAP  LAYER   |      1x16          |  (the output, though can be written as 1x1x16, but is 1-D, i.e.1x16)
*1x1x16*               | *(1x1x16)x10*  |      *1x10*    | (behaves as fully-connected layer for the 1-D data from GAP)


    v. As mentioned earlier, have found better results for the 1x1->GAP option, rather than GAP->1x1(or, FC)
       hence the increments of Batch Normalization, dropout etc are made with this arrangement.
       
    vi. At the end, following Architecture (same as the first-table above, but with increased number of channels,
        like below is found to achieve the required goal: 14,112 params, >99.4% accuracy, in less than 20 epochs 
        (while 'sticking-to' the same learning rate as given in the original code, as Learning-rate tuning is not 
        to be experimented in this session)
        

Input Channels/Image  |  Conv2d/Transform      | Output Channels | RF
---------------------|--------------|----------------------|----------------------
`28x28x1`              | `(3x3x1)x16`   |      `26x26x16`  |      `3x3`
` `              | `BN(16)`   |      ` `  |      ` `
` `              | `Dropout(3%)`   |      ` `  |      ` `
` `              | `ReLU`   |      ` `  |      ` `  
**26x26x16**             | **(3x3x16)x16**  |      **24x24x16** |      **5x5** 
** **             | **BN(16)**   |     ** **  |     ** **
** **             | **Dropout(3%)**   |     ** **  |     ** **
** **             | **ReLU**   |     ** **  |     ** **              
**24x24x16**             | **(3x3x16)x16**  |      **22x22x16** |      **7x7** 
** **             | **BN(16)**   |     ** **  |     ** **
** **             | **Dropout(3%)**   |     ** **  |     ** **
** **             | **ReLU**   |     ** **  |     ** **              
*22x22x16*             |   *MP(2x2)*    |      *11x11x16*   |      *8x8*                      
*11x11x16*             | *(1x1x16)x16*  |      *11x11x16*    |      *8x8*   
** **            | *BN(16)*   |     * *   |     * * 
** **             | *Dropout(3%)*   |     * *   |     * * 
** **             | *ReLU*   |     * *   |     * *                         
**11x11x16**             | **(3x3x16)x24**  |      **9x9x24** |      **12x12**  
** **             | **BN(24)**   |     ** **  |     ** **
** **             | **Dropout(3%)**   |     ** **  |     ** **
** **             | **ReLU**   |     ** **  |     ** **                         
**9x9x24**               | **(3x3x24)x24**  |      **7x7x24**  |      **16x16**   
** **             | **BN(24)**   |     ** **  |     ** **
** **             | **Dropout(3%)**   |     ** **  |     ** **
** **             | **ReLU**   |     ** **  |     ** **                          
*7x7x24*               | *(1x1x24)x10*  |      *7x7x10*    |      *16x16*   (NO RELU at the o/p of this layer)   
7x7x10               | GAP  LAYER   |      1x10          |     
